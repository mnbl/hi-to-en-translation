{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bdbc0f",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of this machine learning project is to develop a robust Hindi-to-English translation model using deep learning techniques. The primary goal is to improve the translation quality for real-time news articles and informational content sourced from various Indian news websites. The model aims to handle the nuances of the Hindi language effectively and produce translations that maintain the contextual and cultural accuracy of the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7f9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import warnings\n",
    "import re\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stanza import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import MarianMTModel, MarianTokenizer, GenerationConfig\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3aec9-2b69-44ea-b781-4a4b5f8abf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1005f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Some non-default generation parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44dd9ee-1004-4d41-afd7-7a3bcd84163f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75259211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Hindi</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.bhaskar.com/local/rajasthan/sikar/</td>\n",
       "      <td>बूथ चलें अभियान; बैनर से दे रहे मतदान जागरूकता का संदेश</td>\n",
       "      <td>Booths run campaign;The message of voting awareness is being given by banner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://navbharattimes.indiatimes.com/business/articlelist/2279786.cms</td>\n",
       "      <td>ईरान-इजरायल तनाव से नया रेकॉर्ड बनाने की तैयारी में सोना, जानिए कहां तक पहुंच सकती है कीमत</td>\n",
       "      <td>Iran-Israeli is preparing to make a new record with stress, know where the price can reach the price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ndtv.in/faith#pfrom=home-khabar_nav</td>\n",
       "      <td>नवरात्रि के तीसरे दिन मां चंद्रघंटा की इस विधि से करें पूजा, मंत्र से लेकर आरती जानें यहां</td>\n",
       "      <td>On the third day of Navratri, do worship with this method of Maa Chandraghanta, know from mantra to Aarti here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://navbharattimes.indiatimes.com/apna-bazaar/fashion/articlelist/80476266.cms</td>\n",
       "      <td>: रुपये से कम में खरीदें ये टॉप क्वालिटी वाली , मिलेगा ताबड़तोड़ डिस्काउंट का ऑफर</td>\n",
       "      <td>: Buy this top quality for less than Rs, you will get a rampant discount offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://navbharattimes.indiatimes.com/state/uttarakhand/articlelist/21236621.cms</td>\n",
       "      <td>मरघट, पनघट सब यहीं... जोशीमठ छोड़ कहीं और नहीं जाएंगे, हाथ में हल-फावड़े लेकर सड़कों पर उतरे पुश्‍तैनी लोग</td>\n",
       "      <td>Marghat, Panaghat all here ... Joshimath will not leave anywhere else, the people who came out on the streets with plow in their hands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.bhaskar.com/local/rajasthan/</td>\n",
       "      <td>करणीमाता मंदिर में श्रद्धालुओं की भीड़ उमड़ी, जाम लगा तो घंटे तक दुपहिया वाहन प्रतापबंध पर रोके</td>\n",
       "      <td>Crowds of devotees gathered in Karnimata temple, two two -wheelers stopped at the glory for hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.amarujala.com/privacy-policy</td>\n",
       "      <td>निजी जानकारी हासिल करना और उसे अपडेट करना</td>\n",
       "      <td>Get personal information and update it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.prabhatkhabar.com/</td>\n",
       "      <td>भगता परब: बोकारो में पीठ छिदवाकर व अंगारों पर चलकर दिखायी आस्था, फीट की ऊंचाई पर झूले भगतिया</td>\n",
       "      <td>Bhagata Parab: Faith shown in Bokaro by piercing back and walking on coals, swing at the height of feet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.prabhatkhabar.com/state/jharkhand/deoghar/cyber-crime-mumbai-police-attacked-in-deoghar-to-save-cyber-criminal-two-soldiers-injured-vehicle-damaged-grj</td>\n",
       "      <td>झारखंड के देवघर में साइबर क्रिमिनल को बचाने के लिए मुंबई पुलिस पर हमला, दो जवान घायल, वाहन क्षतिग्रस्त</td>\n",
       "      <td>Mumbai police attacked, two soldiers injured, vehicle damaged to save cyber criminal in Deoghar, Jharkhand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.punjabkesari.in/business/property</td>\n",
       "      <td>पिछले दो चुनावी वर्षों में मकानों की बिक्री ने नए रिकॉर्ड बनाए</td>\n",
       "      <td>Sale of houses in the last two election years set new records</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                             Reference  \\\n",
       "0                                                                                                                       https://www.bhaskar.com/local/rajasthan/sikar/   \n",
       "1                                                                                               https://navbharattimes.indiatimes.com/business/articlelist/2279786.cms   \n",
       "2                                                                                                                          https://ndtv.in/faith#pfrom=home-khabar_nav   \n",
       "3                                                                                   https://navbharattimes.indiatimes.com/apna-bazaar/fashion/articlelist/80476266.cms   \n",
       "4                                                                                     https://navbharattimes.indiatimes.com/state/uttarakhand/articlelist/21236621.cms   \n",
       "5                                                                                                                             https://www.bhaskar.com/local/rajasthan/   \n",
       "6                                                                                                                             https://www.amarujala.com/privacy-policy   \n",
       "7                                                                                                                                       https://www.prabhatkhabar.com/   \n",
       "8  https://www.prabhatkhabar.com/state/jharkhand/deoghar/cyber-crime-mumbai-police-attacked-in-deoghar-to-save-cyber-criminal-two-soldiers-injured-vehicle-damaged-grj   \n",
       "9                                                                                                                        https://www.punjabkesari.in/business/property   \n",
       "\n",
       "                                                                                                        Hindi  \\\n",
       "0                                                     बूथ चलें अभियान; बैनर से दे रहे मतदान जागरूकता का संदेश   \n",
       "1                  ईरान-इजरायल तनाव से नया रेकॉर्ड बनाने की तैयारी में सोना, जानिए कहां तक पहुंच सकती है कीमत   \n",
       "2                  नवरात्रि के तीसरे दिन मां चंद्रघंटा की इस विधि से करें पूजा, मंत्र से लेकर आरती जानें यहां   \n",
       "3                             : रुपये से कम में खरीदें ये टॉप क्वालिटी वाली , मिलेगा ताबड़तोड़ डिस्काउंट का ऑफर   \n",
       "4  मरघट, पनघट सब यहीं... जोशीमठ छोड़ कहीं और नहीं जाएंगे, हाथ में हल-फावड़े लेकर सड़कों पर उतरे पुश्‍तैनी लोग   \n",
       "5             करणीमाता मंदिर में श्रद्धालुओं की भीड़ उमड़ी, जाम लगा तो घंटे तक दुपहिया वाहन प्रतापबंध पर रोके   \n",
       "6                                                                   निजी जानकारी हासिल करना और उसे अपडेट करना   \n",
       "7                भगता परब: बोकारो में पीठ छिदवाकर व अंगारों पर चलकर दिखायी आस्था, फीट की ऊंचाई पर झूले भगतिया   \n",
       "8      झारखंड के देवघर में साइबर क्रिमिनल को बचाने के लिए मुंबई पुलिस पर हमला, दो जवान घायल, वाहन क्षतिग्रस्त   \n",
       "9                                              पिछले दो चुनावी वर्षों में मकानों की बिक्री ने नए रिकॉर्ड बनाए   \n",
       "\n",
       "                                                                                                                                  English  \n",
       "0                                                            Booths run campaign;The message of voting awareness is being given by banner  \n",
       "1                                    Iran-Israeli is preparing to make a new record with stress, know where the price can reach the price  \n",
       "2                          On the third day of Navratri, do worship with this method of Maa Chandraghanta, know from mantra to Aarti here  \n",
       "3                                                          : Buy this top quality for less than Rs, you will get a rampant discount offer  \n",
       "4  Marghat, Panaghat all here ... Joshimath will not leave anywhere else, the people who came out on the streets with plow in their hands  \n",
       "5                                       Crowds of devotees gathered in Karnimata temple, two two -wheelers stopped at the glory for hours  \n",
       "6                                                                                                  Get personal information and update it  \n",
       "7                                 Bhagata Parab: Faith shown in Bokaro by piercing back and walking on coals, swing at the height of feet  \n",
       "8                              Mumbai police attacked, two soldiers injured, vehicle damaged to save cyber criminal in Deoghar, Jharkhand  \n",
       "9                                                                           Sale of houses in the last two election years set new records  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the web-scrapped csv file\n",
    "df = pd.read_csv(\"./language.csv\")\n",
    "df = shuffle(df).reset_index(drop=True)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248fc45-49b5-455a-9260-3312a02a3227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a2054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking only first 4000 instances\n",
    "df = df[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959d360b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c0097-176d-4bb1-9ddd-1d077a072a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1076baed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reference    0\n",
       "Hindi        0\n",
       "English      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check the null values in dataframe\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a23d7-bd26-44f0-bc0c-3588ffb4cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cccef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_stopwords = [\n",
    "    'है', 'में', 'यह', 'वह', 'के', 'हो', 'को', 'पर', 'इस', 'साथ', 'जो', 'कर', 'था', 'द्वारा', 'होता', 'लिए',\n",
    "    'आप', 'आपको', 'आपका', 'इसे', 'वे', 'उनके', 'बारे', 'तक', 'इन', 'उस', 'अत', 'अब', 'कहा', 'गया', 'जा', 'रहे',\n",
    "    'उनका', 'इसका', 'रहा', 'जैसे', 'सब', 'किस', 'जिस', 'जिसे', 'किसी', 'किन', 'उसका', 'जिन', 'यदि', 'हुआ', 'जब',\n",
    "    'कहीं', 'कौन', 'कौनसा', 'इत्यादि', 'यहाँ', 'वहाँ'\n",
    "]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in hindi_stopwords]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35ac86",
   "metadata": {},
   "source": [
    "* A list named hindi_stopwords that includes common stopwords in Hindi—words which are typically filtered out before processing natural language data because they are frequent and carry minimal meaningful information by themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56ec80-a3d1-469a-9d73-7e660f132261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24b6a03a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHindi_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHindi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\pandas\\core\\series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4780\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Lemmatization\u001b[39;00m\n\u001b[0;32m     19\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m---> 20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Lemmatization\u001b[39;00m\n\u001b[0;32m     19\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m---> 20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1206\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexnames\u001b[38;5;241m.\u001b[39mappend(lexname)\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;66;03m# Load the indices for lemmas and synset offsets\u001b[39;00m\n\u001b[1;32m-> 1206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_lemma_pos_offset_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# load the exception file data into memory\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_exception_map()\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1366\u001b[0m, in \u001b[0;36mWordNetCorpusReader._load_lemma_pos_offset_map\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FILEMAP\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m   1363\u001b[0m \n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;66;03m# parse each line of the file (ignoring comment lines)\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m suffix) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m-> 1366\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fp):\n\u001b[0;32m   1367\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\nltk\\data.py:1152\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Conestoga College\\Desktop\\Conestoga College\\Machine-Learning-Programming\\venv-mlp\\lib\\site-packages\\nltk\\data.py:1143\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m    Read this file's contents, decode them using this reader's\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;124;03m    encoding, and return it as a list of unicode lines.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;124;03m    :param keepends: If false, then strip newlines.\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines(keepends)\n\u001b[1;32m-> 1143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace URLs with a space\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove English and Hindi stopwords\n",
    "    eng_stop_words = set(stopwords.words('english'))\n",
    "    combined_stopwords = eng_stop_words.union(hindi_stopwords)\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    tokens = [word for word in tokens if word not in combined_stopwords]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Hindi_clean'] = df['Hindi'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5226089",
   "metadata": {},
   "source": [
    "* Lowercasing: Converts all characters in the text to lowercase to ensure uniformity.\n",
    "* URL Replacement: Uses a regular expression to find and replace URLs with a space, removing hyperlinks from the text.\n",
    "* Tokenization: Breaks the text into individual words or tokens. This requires importing a tokenizer, which is not explicitly imported in the provided code but typically comes from a library like NLTK.\n",
    "* Stop Words Removal: Removes stopwords from the tokens. Stopwords are commonly used words (like \"and\", \"the\", etc.) that are often filtered out before processing text. The code assumes lists of stopwords for both English and Hindi are available.\n",
    "* Lemmatization: Applies lemmatization to the tokens to reduce them to their base or root form. This part of the code uses the WordNetLemmatizer from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938309d-dc1f-42cb-b2f0-da400afcca66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58297035-7327-429b-ab0e-9122661448d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the first coluumns after cleaning 'Hindi'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2b625-c29a-40f1-a7ce-11b73ac48920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a0cd721",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b35d47",
   "metadata": {},
   "source": [
    "#### Count vectorization:\n",
    "A straightforward method used in text processing to convert text data into a numerical format, which is essential for machine learning models that require numeric input. Here's a simple explanation of how it works:\n",
    "\n",
    "Vocabulary Creation: Count vectorization starts by building a vocabulary of all the unique words in the entire set of documents (text data) you have. Each word in the vocabulary is assigned a unique index.\n",
    "\n",
    "Count Calculation: For each document, the method counts how many times each word from the vocabulary appears. These counts are then organized into a vector (a list of numbers) where each position in the vector corresponds to a word in the vocabulary, and the value at that position is the count of that word in the document.\n",
    "\n",
    "Document Representation: As a result, each document is transformed into a vector of numbers. These vectors will all be of the same length—the size of the vocabulary. Words that appear in a document have counts greater than zero in the vector at their corresponding index, and words that do not appear have a count of zero.\n",
    "\n",
    "[For more information:](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ce53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with a maximum feature number limit\n",
    "vectorizer_bow = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the data\n",
    "bow_features = vectorizer_bow.fit_transform(df['Hindi_clean'])\n",
    "\n",
    "bow_df = pd.DataFrame(bow_features.toarray(), columns=vectorizer_bow.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d3b97",
   "metadata": {},
   "source": [
    "* Setting a Limit (max_features=1000): We tell the tool to only consider the most frequent 1000 words from all the text we give it. This means if there are more than 1000 unique words across all the texts, only the top 1000 by frequency will be used.\n",
    "\n",
    "Transforming Text into Numbers (fit_transform):\n",
    "\n",
    "* Fit: This part of the process is where the tool learns which words are the most common across all the provided Hindi texts. Think of it like making a list of top words.\n",
    "* Transform: After learning, the tool then goes through each text and counts how many times these top words appear. Each text is transformed into a list of numbers, where each number represents how many times a specific word from the top words list appears in the text.\n",
    "* Creating a Data Table (DataFrame): The lists of numbers for each text are organized into a table where each column represents one of the top words, and each row represents one of the texts. The numbers in the table tell us the count of each word for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69986fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d76b800",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Training Data: Word2Vec requires a large amount of text data to learn from. This text data helps the model understand the context in which words appear.\n",
    "\n",
    "Concept of Context: The model learns by looking at the words that frequently appear around a given word. For instance, in the sentence \"I love eating apples\", if the model is focusing on the word \"eating\", the surrounding words like \"love\" and \"apples\" give clues about its meaning.\n",
    "\n",
    "Creating Vectors: Through training, Word2Vec assigns each word in the vocabulary a vector (a list of numbers). These vectors are created in such a way that they capture relationships and patterns among words in the text data. Words with similar meanings end up having vectors that are close to each other in the vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d23255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts for Word2Vec\n",
    "sentences = [row.split() for row in df['Hindi_clean']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert sentences to vectors\n",
    "def get_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    vector = sum(word2vec_model.wv[word] for word in words if word in word2vec_model.wv) / len(words)\n",
    "    return vector\n",
    "\n",
    "X_word2vec = np.array([get_vector(sentence) for sentence in df['Hindi_clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049b10f-7d3b-44d5-b0ce-82bf4d73390b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ccaf6-dc8f-4509-9248-24f15872259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, GenerationConfig\n",
    "\n",
    "# Load your model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-hi-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Define a GenerationConfig with your custom parameters\n",
    "gen_config = GenerationConfig(\n",
    "    max_length=512,\n",
    "    num_beams=6,\n",
    "    bad_words_ids=[[61126]],\n",
    "    forced_eos_token_id=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fee77-d6ee-411d-8755-45456b6d2f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef1480-b00d-40c8-960e-e65e30a7b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "model = MarianMTModel.from_pretrained(save_directory)\n",
    "tokenizer = MarianTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "def translate(text):\n",
    "    # Encode the text input\n",
    "    encoded_hindi_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Generate translation ids\n",
    "    translated_ids = model.generate(**encoded_hindi_text)\n",
    "    \n",
    "    # Decode the translated text and return it\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d66ea-60c1-43cc-a22b-c3558614211c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e30fc-4970-462d-ad3c-c023091e2181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbba56-a220-4752-ac2b-96472440e13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff2b25-7671-477a-902f-5d1e82aa57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X and Y\n",
    "X = df[\"Hindi_clean\"]\n",
    "Y = df[\"English\"]\n",
    "\n",
    "# Splitting the data into train, validation, and test sets\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Confirm the splits\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Preparing data for model training\n",
    "train_encodings = tokenizer(X_train.tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "val_encodings = tokenizer(X_val.tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "train_labels = tokenizer(Y_train.tolist(), return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "val_labels = tokenizer(Y_val.tolist(), return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "\n",
    "# Define the dataset class for PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = TranslationDataset(train_encodings, train_labels)\n",
    "val_dataset = TranslationDataset(val_encodings, val_labels)\n",
    "\n",
    "# Training setup\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=8,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753b1c2-3d0c-4e9d-9454-e9e6ea679924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d61bc-8fa0-4ed8-b1e0-132a65e30532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(texts, model, tokenizer, device, max_length=128):\n",
    "    encodings = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    encodings = {key: val.to(device) for key, val in encodings.items()}  # Move encodings to the device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        translated_tokens = model.generate(**encodings)\n",
    "    \n",
    "    translations = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens.cpu()]  \n",
    "    return translations\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "sample_df = df.sample(5)\n",
    "# Batch processing\n",
    "batch_size = 1  # Adjust based on your system's memory\n",
    "translations = []\n",
    "for i in range(0, len(sample_df), batch_size):\n",
    "    batch_texts = sample_df['Hindi_clean'].iloc[i:i+batch_size].tolist()\n",
    "    batch_translations = translate_batch(batch_texts, model, tokenizer, device)\n",
    "    translations.extend(batch_translations)\n",
    "\n",
    "\n",
    "sample_df['predicted_translation'] = translations\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2b1f5-2a70-4403-a77c-26947c255536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model, tokenizer, and generation configuration\n",
    "save_directory = \"./saved_model\"\n",
    "model.save_pretrained(save_directory, gen_config=gen_config)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8aa70b-9eeb-4fe1-9106-b162fe87130d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
